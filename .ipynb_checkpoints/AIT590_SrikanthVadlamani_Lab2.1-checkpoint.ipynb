{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Srikanth Vadlamani\n",
    "### Professor: Liao\n",
    "### AIT-590-B01\n",
    "#### Assignment: Optional Lab 2.1\n",
    "------------------------------------------------------------------------------\n",
    "#### Description: The code runs in Python3. \n",
    "\n",
    "#### Citations\n",
    "    https://mymasonportal.gmu.edu/bbcswebdav/pid-10725131-dt-content-rid-183551030_1/courses/43749.202040/Liao_NLP_text_processing%281%29.html\n",
    "    https://mymasonportal.gmu.edu/bbcswebdav/pid-10727167-dt-content-rid-183551060_1/xid-183551060_1\n",
    "    https://stackabuse.com/python-for-nlp-creating-tf-idf-model-from-scratch/\n",
    "    https://www.datacamp.com/community/tutorials/absolute-weighted-word-frequency\n",
    "    https://stackabuse.com/text-summarization-with-nltk-in-python/\n",
    "    https://stackabuse.com/python-for-nlp-creating-tf-idf-model-from-scratch/\n",
    "    Also, the review notes from Professor Liao."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Librariries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs  # BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Scrape the textdata from webpage with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _scrape_webpage(url):\n",
    "    \"\"\"\n",
    "    Use BeautifulSoup to scrape the webpage text contents.\n",
    "    \"\"\"    \n",
    "    scraped_textdata = urllib.request.urlopen(url)\n",
    "    textdata = scraped_textdata.read()\n",
    "    parsed_textdata = bs.BeautifulSoup(textdata,'lxml')\n",
    "    paragraphs = parsed_textdata.find_all('p')\n",
    "    formated_text = \"\"\n",
    "\n",
    "    for para in paragraphs:\n",
    "        formated_text += para.text\n",
    "    \n",
    "    return formated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext = _scrape_webpage('https://en.wikipedia.org/wiki/Natural_language_processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Punctuation and Digits\n",
    "##### 2.1 Tokenize the words\n",
    "##### 2.2 Remove the stop words, punctuation,anddigit numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing NLP is a subfield of linguistics computer science information engineering and artificial intelligence concerned with the interactions between computers and human natural languages in particular how to program computers to process and analyze large amounts of natural language data\n",
      "Challenges in natural language processing frequently involve speech recognition natural language understanding and natural language generation\n",
      "The history of natural language processing NLP generally started in the s although work can be found from earlier periods\n",
      "In  Alan Turing published an article titled Computing Machinery and Intelligence which proposed what is now called the Turing test as a criterion of intelligenceclarification needed\n",
      "The Georgetown experiment in  involved fully automatic translation of more than sixty Russian sentences into English The authors claimed that within three or five years machine translation would be a solved problem  However real progress was much slower and after the ALPAC report in  which found that tenyearlong research had failed to fulfill the expectations funding for machine translation was dramatically reduced  Little further research in machine translation was conducted until the late s when the first statistical machine translation systems were developed\n",
      "Some notably successful natural language processing systems developed in the s were SHRDLU a natural language system working in restricted blocks worlds with restricted vocabularies and ELIZA a simulation of a Rogerian psychotherapist written by Joseph Weizenbaum between  and   Using almost no information about human thought or emotion ELIZA sometimes provided a startlingly humanlike interaction When the patient exceeded the very small knowledge base ELIZA might provide a generic response for example responding to My head hurts with Why do you say your head hurts\n",
      "During the s many programmers began to write conceptual ontologies which structured realworld information into computerunderstandable data  Examples are MARGIE Schank  SAM Cullingford  PAM Wilensky  TaleSpin Meehan  QUALM Lehnert  Politics Carbonell  and Plot Units Lehnert   During this time many chatterbots were written including PARRY Racter and Jabberwacky\n",
      "Up to the s most natural language processing systems were based on complex sets of handwritten rules  Starting in the late s however there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing  This was due to both the steady increase in computational power see Moores law and the gradual lessening of the dominance of Chomskyan theories of linguistics eg transformational grammar whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machinelearning approach to language processing Some of the earliestused machine learning algorithms such as decision trees produced systems of hard ifthen rules similar to existing handwritten rules  However partofspeech tagging introduced the use of hidden Markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching realvalued weights to the features making up the input data The cache language models upon which many speech recognition systems now rely are examples of such statistical models  Such models are generally more robust when given unfamiliar input especially input that contains errors as is very common for realworld data and produce more reliable results when integrated into a larger system comprising multiple subtasks\n",
      "Many of the notable early successes occurred in the field of machine translation due especially to work at IBM Research where successively more complicated statistical models were developed  These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government  However most other systems depended on corpora specifically developed for the tasks implemented by these systems which was and often continues to be a major limitation in the success of these systems As a result a great deal of research has gone into methods of more effectively learning from limited amounts of data\n",
      "Recent research has increasingly focused on unsupervised and semisupervised learning algorithms  Such algorithms can learn from data that has not been handannotated with the desired answers or using a combination of annotated and nonannotated data  Generally this task is much more difficult than supervised learning and typically produces less accurate results for a given amount of input data  However there is an enormous amount of nonannotated data available including among other things the entire content of the World Wide Web which can often make up for the inferior results if the algorithm used has a low enough time complexity to be practical\n",
      "In the s representation learning and deep neural networkstyle machine learning methods became widespread in natural language processing due in part to a flurry of results showing that such techniques can achieve stateoftheart results in many natural language tasks for example in language modeling parsing and many others Popular techniques include the use of word embeddings to capture semantic properties of words and an increase in endtoend learning of a higherlevel task eg question answering instead of relying on a pipeline of separate intermediate tasks eg partofspeech tagging and dependency parsing In some areas this shift has entailed substantial changes in how NLP systems are designed such that deep neural networkbased approaches may be viewed as a new paradigm distinct from statistical natural language processing For instance the term neural machine translation NMT emphasizes the fact that deep learningbased approaches to machine translation directly learn sequencetosequence transformations obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation SMT\n",
      "In the early days many languageprocessing systems were designed by handcoding a set of rules such as by writing grammars or devising heuristic rules for stemming \n",
      "Since the socalled statistical revolution in the late s and mids much natural language processing research has relied heavily on machine learning The machinelearning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora the plural form of corpus is a set of documents possibly with human or computer annotations of typical realworld examples\n",
      "Many different classes of machinelearning algorithms have been applied to naturallanguageprocessing tasks These algorithms take as input a large set of features that are generated from the input data Some of the earliestused algorithms such as decision trees produced systems of hard ifthen rules similar to the systems of handwritten rules that were then common Increasingly however research has focused on statistical models which make soft probabilistic decisions based on attaching realvalued weights to each input feature Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one producing more reliable results when such a model is included as a component of a larger system\n",
      "Systems based on machinelearning algorithms have many advantages over handproduced rules\n",
      "The following is a list of some of the most commonly researched tasks in natural language processing Some of these tasks have direct realworld applications while others more commonly serve as subtasks that are used to aid in solving larger tasks\n",
      "Though natural language processing tasks are closely intertwined they are frequently subdivided into categories for convenience A coarse division is given below\n",
      "The first published work by an artificial intelligence was published in   the Road marketed as a novel contains sixty million words\n",
      "Cognition refers to the mental action or process of acquiring knowledge and understanding through thought experience and the senses Cognitive science is the interdisciplinary scientific study of the mind and its processes Cognitive linguistics is an interdisciplinary branch of linguistics combining knowledge and research from both psychology and linguistics George Lakoff offers a methodology to build Natural language processing NLP algorithms through the perspective of Cognitive science along with the findings of Cognitive linguistics \n",
      "The first defining aspect of this cognitive task of NLP is the application of the theory of Conceptual metaphor explained by Lakoff as “the understanding of one idea in terms of another” which provides an idea of the intent of the author \n",
      "For example consider some of the meanings in English of the word “big” When used as a Comparative as in “That is a big tree” a likely inference of the intent of the author is that the author is using the word “big” to imply a statement about the tree being ”physically large” in comparison to other trees or the authors experience  When used as a Stative verb as in ”Tomorrow is a big day” a likely inference of the author’s intent it that ”big” is being used to imply ”importance”  These examples are not presented to be complete but merely as indicators of the implication of the idea of Conceptual metaphor  The intent behind other usages like in ”She is a big person” will remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional information  \n",
      "This leads to the second defining aspect of this cognitive task of NLP namely Probabilistic contextfree grammar PCFG which enables cognitive NLP algorithms to assign relative measures of meaning  to a word phrase sentence or piece of text based on the information presented before and after the piece of text being analyzed The mathematical equation for such algorithms is presented in US patent   \n",
      "Where\n",
      "     RMM is the Relative Measure of Meaning\n",
      "     token is any block of text sentence phrase or word\n",
      "     N is the number of tokens being analyzed\n",
      "     PMM is the Probable Measure of Meaning based on a corpora\n",
      "     n is one less than the number of tokens being analyzed\n",
      "     d is the location of the token along the sequence of n tokens\n",
      "     PF is the Probability Function specific to a language\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def remove_punctation(str):\n",
    "    return ''.join(c for c in str if c not in punctuation)\n",
    "\n",
    "txtNoPuctuation = remove_punctation(mytext)\n",
    "\n",
    "\n",
    "txtNoNumbers = ''.join(c for c in txtNoPuctuation if not c.isdigit())\n",
    "print(txtNoNumbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Stop words\n",
    "##### 2.2 Remove the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', 'NLP', 'subfield', 'linguistics', 'computer', 'science', 'information', 'engineering', 'artificial', 'intelligence', 'concerned', 'interactions', 'computers', 'human', 'natural', 'languages', 'particular', 'program', 'computers', 'process', 'analyze', 'large', 'amounts', 'natural', 'language', 'data', 'Challenges', 'natural', 'language', 'processing', 'frequently', 'involve', 'speech', 'recognition', 'natural', 'language', 'understanding', 'natural', 'language', 'generation', 'The', 'history', 'natural', 'language', 'processing', 'NLP', 'generally', 'started']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text_tokens = word_tokenize(txtNoNumbers)\n",
    "\n",
    "tokens_without_sw = [token for token in text_tokens if token not in stopwords.words('english')]\n",
    "print(tokens_without_sw[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Lemmatize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', 'NLP', 'subfield', 'linguistics', 'computer', 'science', 'information', 'engineering', 'artificial', 'intelligence', 'concerned', 'interaction', 'computer', 'human', 'natural', 'language', 'particular', 'program', 'computer', 'process', 'analyze', 'large', 'amount', 'natural', 'language', 'data', 'Challenges', 'natural', 'language', 'processing', 'frequently', 'involve', 'speech', 'recognition', 'natural', 'language', 'understanding', 'natural', 'language', 'generation', 'The', 'history', 'natural', 'language', 'processing', 'NLP', 'generally', 'started']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in tokens_without_sw]\n",
    "print (lemmatized_word[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating N-grams with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def extract_ngrams(data, num):\n",
    "    n_grams = ngrams(data, num)\n",
    "    return [' '.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_num = 3\n",
    "# data[\"text\"] = [preprocess_text(t) for t in data[\"text\"]]\n",
    "n_grams = extract_ngrams(tokens_without_sw, gram_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Natural language processing': 0.18181818181818182,\n",
       " 'language processing NLP': 0.2727272727272727,\n",
       " 'processing NLP subfield': 0.09090909090909091,\n",
       " 'NLP subfield linguistics': 0.09090909090909091,\n",
       " 'subfield linguistics computer': 0.09090909090909091,\n",
       " 'linguistics computer science': 0.09090909090909091,\n",
       " 'computer science information': 0.09090909090909091,\n",
       " 'science information engineering': 0.09090909090909091,\n",
       " 'information engineering artificial': 0.09090909090909091,\n",
       " 'engineering artificial intelligence': 0.09090909090909091,\n",
       " 'artificial intelligence concerned': 0.09090909090909091,\n",
       " 'intelligence concerned interactions': 0.09090909090909091,\n",
       " 'concerned interactions computers': 0.09090909090909091,\n",
       " 'interactions computers human': 0.09090909090909091,\n",
       " 'computers human natural': 0.09090909090909091,\n",
       " 'human natural languages': 0.09090909090909091,\n",
       " 'natural languages particular': 0.09090909090909091,\n",
       " 'languages particular program': 0.09090909090909091,\n",
       " 'particular program computers': 0.09090909090909091,\n",
       " 'program computers process': 0.09090909090909091,\n",
       " 'computers process analyze': 0.09090909090909091,\n",
       " 'process analyze large': 0.09090909090909091,\n",
       " 'analyze large amounts': 0.09090909090909091,\n",
       " 'large amounts natural': 0.09090909090909091,\n",
       " 'amounts natural language': 0.09090909090909091,\n",
       " 'natural language data': 0.09090909090909091,\n",
       " 'language data Challenges': 0.09090909090909091,\n",
       " 'data Challenges natural': 0.09090909090909091,\n",
       " 'Challenges natural language': 0.09090909090909091,\n",
       " 'natural language processing': 1.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculated weighted frequency using lemmatized_word\n",
    "wt_frequencies = {}\n",
    "for word in n_grams:\n",
    "    if word not in wt_frequencies.keys():\n",
    "        wt_frequencies[word] = 1\n",
    "    else:\n",
    "        wt_frequencies[word] += 1\n",
    "\n",
    "maximum_frequncy = max(wt_frequencies.values())\n",
    "\n",
    "for word in wt_frequencies.keys():\n",
    "    wt_frequencies[word] = (wt_frequencies[word]/maximum_frequncy)\n",
    "\n",
    "  \n",
    "dict(list(wt_frequencies.items())[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.': 108.27272727272911,\n",
       " 'Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.': 108.27272727272911,\n",
       " 'The history of natural language processing (NLP) generally started in the 1950s, although work can be found from earlier periods.': 108.27272727272911,\n",
       " 'In 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence[clarification needed].': 108.27272727272911,\n",
       " 'The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English.': 108.27272727272911,\n",
       " 'The authors claimed that within three or five years, machine translation would be a solved problem.': 108.27272727272911,\n",
       " '[2]  However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced.': 108.27272727272911,\n",
       " 'Little further research in machine translation was conducted until the late 1980s when the first statistical machine translation systems were developed.': 108.27272727272911,\n",
       " 'Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966.': 108.27272727272911,\n",
       " 'Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction.': 108.27272727272911}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentence_tokens = sent_tokenize(mytext)\n",
    "sentence_tokens[:10]\n",
    "\n",
    "sentence_score = {}\n",
    "for sent in sentence_tokens:\n",
    "#     for wrd in nltk.word_tokenize(sent.lower()):\n",
    "    for wrd in n_grams:\n",
    "        if wrd in wt_frequencies.keys():\n",
    "            if len(wrd.split(' ')) < 25:\n",
    "                if sent not in sentence_score.keys():\n",
    "                    sentence_score[sent] = wt_frequencies[wrd]\n",
    "                else:\n",
    "                    sentence_score[sent] += wt_frequencies[wrd]\n",
    "dict(list(sentence_score.items())[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>=' not supported between instances of 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-f78360705ef6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mheapq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msentence_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msummary_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheapq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_grams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentence_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/heapq.py\u001b[0m in \u001b[0;36mnlargest\u001b[0;34m(n, iterable, key)\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>=' not supported between instances of 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "sentence_num = 3\n",
    "summary_sentence = heapq.nlargest(n_grams, sentence_score, key=sentence_score.get)\n",
    "summary=' '.join(summary_sentence)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
