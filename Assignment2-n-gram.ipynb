{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFQDist(N, inputCorpus):\n",
    "    ngram_fqdist, nm1gram_freq_dist, sentence_tokens = [], [], []\n",
    "    # read all files and create tokens\n",
    "    for fName in inputCorpus:\n",
    "        with io.open(fName, encoding=\"utf-8\") as fp:\n",
    "            tmpSentenceToken = sent_tokenize(fp.read())\n",
    "            # Process each sentence\n",
    "            for sentence in tmpSentenceToken:\n",
    "                # Process each word in the sentence\n",
    "                if N > 1:\n",
    "                    wTkn = re.findall(r\"[\\w]+|[^\\s\\w]\", \"BGN \"+sentence+\" EOF\")\n",
    "                else:\n",
    "                    wTkn = re.findall(r\"[\\w]+|[^\\s\\w]\", sentence+\" EOF\")\n",
    "\n",
    "                for wtk in wTkn:\n",
    "                    sentence_tokens.append(wtk)\n",
    "\n",
    "        logger.info(\"processing: \"+ str(N) +\"-gram\")\n",
    "        n1gram = generateNgrams(N, sentence_tokens)\n",
    "        ngram_fqdist = nltk.FreqDist(n1gram)\n",
    "        logging.info(len(ngram_fqdist))\n",
    "\n",
    "        # Only process when N-Gram 2 or more\n",
    "        if N > 1:\n",
    "            logger.info(\"processing: \"+ str(N-1) +\"-gram\")\n",
    "            n1Minus1Gram = generateNgrams(N-1, sentence_tokens[0:-1])\n",
    "            nm1gram_freq_dist = nltk.FreqDist(n1Minus1Gram)\n",
    "            logging.info(len(nm1gram_freq_dist))\n",
    "           \n",
    "\n",
    "    return ngram_fqdist, nm1gram_freq_dist, sentence_tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sentences based on unigram\n",
    "def genAndShowUnigram(M, ngram_fqdist, sentence_tokens):\n",
    "    \n",
    "    uniFreqList = []\n",
    "    \n",
    "    temp = 0   \n",
    "    \n",
    "    # build a nested list to store [(token, cumulative relative frequency),...]\n",
    "    for k,v in ngram_fqdist.items():\n",
    "        temp = temp + v/len(sentence_tokens)\n",
    "        uniFreqList.append([k, temp])      \n",
    "\n",
    "    for m in range(0, M):        \n",
    "        sentence, word = '', ''\n",
    "        notEnd = True\n",
    "                   \n",
    "        while(notEnd):              \n",
    "            prob = random.uniform(0, 1)  # assign a random number between 0 and 1\n",
    "            \n",
    "            # use binary search to find where \"prob\" falls\n",
    "            left = 0\n",
    "            right = len(uniFreqList) - 1  \n",
    "              \n",
    "            while (right - left > 1):\n",
    "                mid = int((left + right) / 2)\n",
    "                \n",
    "                if uniFreqList[mid][1] > prob:\n",
    "                    right = mid\n",
    "                elif uniFreqList[mid][1] < prob:\n",
    "                    left = mid\n",
    "                else: \n",
    "                    word = uniFreqList[mid][0]\n",
    "            \n",
    "            if uniFreqList[left][1] > prob:\n",
    "                word = uniFreqList[left][0]\n",
    "            elif uniFreqList[left][1] < prob:\n",
    "                word = uniFreqList[right][0]\n",
    "            else: \n",
    "                word = uniFreqList[left][0]\n",
    "            \n",
    "            if word in string.punctuation and len(word_tokenize(sentence)) == 0: # if the first word is a punctuation , then skip it\n",
    "                continue\n",
    "            elif word == 'EOF' and len(word_tokenize(sentence)) <= 20:    # if the word is 'EOF', but the sentence is less than 20 words, then skip it          \n",
    "                continue\n",
    "            else:\n",
    "                sentence = sentence + \" \" + word\n",
    "                \n",
    "                # if the selecte word == \"end\", then end the setence creation\n",
    "                if word == 'EOF':  # make sure the sentence has at least 1 word. \n",
    "                    notEnd = False\n",
    "        \n",
    "        sentence = re.sub(r'EOF', '', sentence)   # remove \" EOF\" from the sentence  \n",
    "        print(\"Sentence\", m + 1, \": \", sentence)\n",
    "        print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use zip function to create N-gram based on tokens\n",
    "def generateNgrams(N, tokens):\n",
    "    # print(N)\n",
    "    # print(tokens)\n",
    "    ngrams_list = zip(*[tokens[i:] for i in range(N)])\n",
    "\n",
    "    return [\" \".join(ngram) for ngram in ngrams_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The method will return relative frequency for N ad N-1 gram tokens.\n",
    "# @N - Number of n-grams\n",
    "# @inputCorpus - the name of the files to train the model\n",
    "#\n",
    "def generateFQDist(N, inputCorpus):\n",
    "    ngram_fqdist, nm1gram_freq_dist, sentence_tokens = [], [], []\n",
    "    # read all files and create tokens\n",
    "    for fName in inputCorpus:\n",
    "        with io.open(fName, encoding=\"utf-8\") as fp:\n",
    "            tmpSentenceToken = sent_tokenize(fp.read())\n",
    "            # Process each sentence\n",
    "            for sentence in tmpSentenceToken:\n",
    "                # Process each word in the sentence\n",
    "                if N > 1:\n",
    "                    wTkn = re.findall(r\"[\\w]+|[^\\s\\w]\", \"BGN \"+sentence+\" EOF\")\n",
    "                else:\n",
    "                    wTkn = re.findall(r\"[\\w]+|[^\\s\\w]\", sentence+\" EOF\")\n",
    "\n",
    "                for wtk in wTkn:\n",
    "                    sentence_tokens.append(wtk)\n",
    "\n",
    "        logger.info(\"processing: \"+ str(N) +\"-gram\")\n",
    "        n1gram = generateNgrams(N, sentence_tokens)\n",
    "        ngram_fqdist = nltk.FreqDist(n1gram)\n",
    "        logging.info(len(ngram_fqdist))\n",
    "\n",
    "        # Only process when N-Gram 2 or more\n",
    "        if N > 1:\n",
    "            logger.info(\"processing: \"+ str(N-1) +\"-gram\")\n",
    "            n1Minus1Gram = generateNgrams(N-1, sentence_tokens[0:-1])\n",
    "            nm1gram_freq_dist = nltk.FreqDist(n1Minus1Gram)\n",
    "            logging.info(len(nm1gram_freq_dist))\n",
    "           \n",
    "\n",
    "    return ngram_fqdist, nm1gram_freq_dist, sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the given word/words\n",
    "def get_previous_n_words(i, N, my_tokens):\n",
    "    temp = []\n",
    "    for j in range(1, N):\n",
    "        temp.append(my_tokens[i - j])\n",
    "    # why\n",
    "    temp.reverse()\n",
    "    previous_n_words = ' '.join(map(str, temp))\n",
    "    return previous_n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reltive_frq(N, ngram_fqdist, nm1gram_freq_dist, sentence_tokens):\n",
    "    # variables to store N-Gram relative frequency and N-1 gram Relative Frequency\n",
    "    dict_ngram, dict_nm1gram = {}, {}\n",
    "\n",
    "    # print(len(sentence_tokens))\n",
    "    # print(len(ngram_fqdist))\n",
    "    # print(len(nm1gram_freq_dist))\n",
    "\n",
    "    # Check lecture slide 50 in Lecture 4 - Ngrams_rev(1) for explanation\n",
    "    for key, value in ngram_fqdist.items():\n",
    "        dict_ngram[key] = value/len(ngram_fqdist)\n",
    "\n",
    "    relative_frequency_dictionary = {}\n",
    "    if N > 1:\n",
    "        # Check lecture slide 50 in Lecture 4 - Ngrams_rev(1) for explanation\n",
    "        for key, value in nm1gram_freq_dist.items():\n",
    "            dict_nm1gram[key] = value/len(nm1gram_freq_dist)\n",
    "\n",
    "        # formula for relative frequency freq(Xk-1, Xk)/freq(Xk-1)\n",
    "        for i in range(N - 1, len(sentence_tokens)):  # iterate all the tokens\n",
    "            previous_n_words = get_previous_n_words(i, N, sentence_tokens)\n",
    "            # print(i, condi_token)\n",
    "\n",
    "            relative_frequency_dictionary[(sentence_tokens[i], previous_n_words)] = dict_ngram[previous_n_words + \" \" + sentence_tokens[i]]/dict_nm1gram[previous_n_words]\n",
    "\n",
    "    return relative_frequency_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predictNextWord(next_keys, relative_frequency_dictionary):\n",
    "    total_probability, temp = 0, 0\n",
    "    word = ''\n",
    "    select_list = []\n",
    "\n",
    "    # calculate the sum of the probability\n",
    "    for key in next_keys:\n",
    "        total_probability += relative_frequency_dictionary[key]\n",
    "\n",
    "    # normalize and calculate the cumulative relative probability.\n",
    "    # create a list = [[prediction, given, cumulative relative probability] ... ]\n",
    "    for key in next_keys:\n",
    "        temp = temp + relative_frequency_dictionary[key] / total_probability\n",
    "        select_list.append([key[0], key[1], temp])\n",
    "\n",
    "    prob = random.uniform(0, 1)  # assign a random number between 0 and 1\n",
    "\n",
    "    # use binary search to find where \"prob\" falls\n",
    "    left = 0\n",
    "    right = len(select_list) - 1\n",
    "\n",
    "    while (right - left > 1):\n",
    "        mid = int((left + right) / 2)\n",
    "\n",
    "        if select_list[mid][2] > prob:\n",
    "            right = mid\n",
    "        elif select_list[mid][2] < prob:\n",
    "            left = mid\n",
    "        else:\n",
    "            word = select_list[mid][0]\n",
    "\n",
    "    if select_list[left][2] > prob:\n",
    "        word = select_list[left][0]\n",
    "    elif select_list[left][2] < prob:\n",
    "        word = select_list[right][0]\n",
    "    else:\n",
    "        word = select_list[left][0]\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sentences for N-gram\n",
    "def generateRandomSentences(N, M, relative_frequency_dictionary):\n",
    "    for m in range(0, M):\n",
    "        start_keys = []\n",
    "\n",
    "        # Find the keys that start with BGN. Example, \"BGN the\" or \"BGN every\".\n",
    "        # Also, relative frequency is not 0, which implies the word is used elsewhere.\n",
    "        start_keys = [key for key in relative_frequency_dictionary.keys() if 'BGN' == key[1].split(\" \")[0] and relative_frequency_dictionary[key] != 0]\n",
    "        # print(\"start_keys: \", start_keys)\n",
    "\n",
    "        # Choose a random second word that starting key word from above step. Slide 22 of 40 from W2 review class\n",
    "        # For example, get keys with key \"every person in\"\n",
    "        first_word = random.choice(start_keys)[1]\n",
    "        # print()\n",
    "        # print(\"sentence's first_word: \", first_word)\n",
    "\n",
    "        notEnd = True\n",
    "\n",
    "        # The first word is now \" every \"\n",
    "        sentence = first_word\n",
    "\n",
    "        while (notEnd):\n",
    "            tokens = sentence.split()\n",
    "            given = tokens[-(N - 1):]\n",
    "            # Get the next word after the first word\n",
    "            given = ' '.join(map(str, given))  # create given word/words\n",
    "            # print(\"given tokens: \", given)\n",
    "\n",
    "            # find subsequent words with word from previous step and relative frequency is not 0\n",
    "            next_keys = [key for key in relative_frequency_dictionary.keys() if given == key[1]]\n",
    "            # print(\"next_keys: \", next_keys)\n",
    "\n",
    "            # re-arrange words according the relative frequency  Slide 28 of 40 from W2 review class\n",
    "            next_word = predictNextWord(next_keys, relative_frequency_dictionary)\n",
    "            # print(\"next_word: \", next_word)\n",
    "\n",
    "            sentence = sentence + \" \" + next_word  # add the selected word to the sentence\n",
    "\n",
    "            # if the selecte word includes \"EOF\", then end the setence creation\n",
    "            if next_word == 'EOF':\n",
    "                notEnd = False\n",
    "\n",
    "        sentence = re.sub(r'BGN', '', sentence)  # remove \"BGN\" from the sentence\n",
    "        sentence = re.sub(r'EOF', '', sentence)  # remove \"EOF\" from the sentence\n",
    "        print(\"Sentence\", m + 1, \": \", sentence)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 :   “ Revenge ? \n",
      "\n",
      "Sentence 2 :   The Squire ' s nature , gave a preposterous din that persons aboard our ship . \n",
      "\n",
      "Sentence 3 :   This picture is here . ” “ Never mind that Tom Sawyer he says . ” I says , dropping the knife to fulfil the regulation price , sure ; and the two adventurers crept in under it , they piled on it again ; but the accumulated fatigue of days more and then set aside , turned a deaf and dumb one - - just depend on that . \n",
      "\n",
      "Sentence 4 :   We will go to the rear mule , or totally distrust ; his president cannot appoint him to fly and seek further . \n",
      "\n",
      "Sentence 5 :   His committee had instructed him to change the subject ? \n",
      "\n",
      "Sentence 6 :   Seven blows - - went away and ever . \n",
      "\n",
      "Sentence 7 :   oh , it was offered , the people in the cote - house fell to rocking himself back and your neighbors in the dark , now , to plant my right hand . \n",
      "\n",
      "Sentence 8 :   He can see that there was no mean timber whereof to construct dangers out of the window - sash and stuff the straw tick better than him ; I shall go for howling adventures amongst the bushes . \n",
      "\n",
      "Sentence 9 :   Often we felt repaid for my revenge that I was a moat to this church are the ugliest , wickedest looking villains we have used - - perfect during the hey - day as soon as I sat down ; a funeral , along the avenue , and looks wiser than the worms . \n",
      "\n",
      "Sentence 10 :   Articles of interest in the way to keep him from a window , where the road was tunneled through a small , steep banks on the breasts with one Thumb . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "from os import path\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "import io\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "punctuations = list(string.punctuation)\n",
    "import logging\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "log_format = '%(asctime)s %(filename)s: %(message)s'\n",
    "logging.basicConfig(filename=\"ngram-log.txt\", format=log_format,\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "logger.info(\"=====N-Gram execution Start==========\")\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    inputCorpus = ['twain1.txt','twain2.txt','twain3.txt']\n",
    "    N = 3\n",
    "    M = 10\n",
    "\n",
    "    start_time = time.localtime()  # get start time\n",
    "\n",
    "    ngram_fqdist, nm1gram_freq_dist, sentence_tokens = generateFQDist(N, inputCorpus)\n",
    "\n",
    "    if N > 1:\n",
    "        relative_frequency_dictionary = calculate_reltive_frq(N, ngram_fqdist, nm1gram_freq_dist, sentence_tokens)\n",
    "        # logger.info(relative_frequency_dictionary)\n",
    "        generateRandomSentences(N, M, relative_frequency_dictionary)\n",
    "    else:\n",
    "        genAndShowUnigram(M, ngram_fqdist, sentence_tokens)\n",
    "\n",
    "    ### log processing\n",
    "    process_time = time.mktime(time.localtime()) - time.mktime(start_time)  # get process time in seconds\n",
    "\n",
    "    logging.info(\"%s minutes, python ngram.py %d\", round(process_time / 60, 5), N)  # log process time in minutes\n",
    "\n",
    "    logging.info('%exit')  # exit logging\n",
    "    ### log end\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
